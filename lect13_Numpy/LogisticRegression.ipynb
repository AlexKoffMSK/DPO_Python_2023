{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate = 1, num_iterations = 2000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = []\n",
    "        self.b = 0\n",
    "        \n",
    "    def initialize_weight(self,dim):\n",
    "        \"\"\"\n",
    "        This function creates a vector of zeros of shape (dim, 1)      for w and initializes b to 0.\n",
    "        Argument:\n",
    "        dim -- size of the w vector we want (or number of parameters  in this case)\n",
    "        \"\"\"\n",
    "        w = np.zeros((dim,1))\n",
    "        b = 0\n",
    "        return w, b\n",
    "\n",
    "    def sigmoid(self,z):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid of z\n",
    "        Argument:\n",
    "        z -- is the decision boundary of the classifier\n",
    "        \"\"\"\n",
    "        s = 1/(1 + np.exp(-z)) \n",
    "        return s\n",
    "    \n",
    "    def hypothesis(self,w,X,b):\n",
    "        \"\"\"\n",
    "        This function calculates the hypothesis for the present model\n",
    "        Argument:\n",
    "         w -- weight vector\n",
    "         X -- The input vector\n",
    "         b -- The bias vector\n",
    "        \"\"\"\n",
    "        H = self.sigmoid(np.dot(w.T,X)+b) \n",
    "        return H\n",
    "    \n",
    "    def cost(self,H,Y,m):\n",
    "        \"\"\"\n",
    "        This function calculates the cost of hypothesis\n",
    "        Arguments: \n",
    "         H -- The hypothesis vector \n",
    "         Y -- The output \n",
    "         m -- Number training samples\n",
    "        \"\"\"\n",
    "        cost = -np.sum(Y*np.log(H)+ (1-Y)*np.log(1-H))/m \n",
    "        cost = np.squeeze(cost)   \n",
    "        return cost\n",
    "    \n",
    "    def cal_gradient(self, w,H,X,Y):\n",
    "        \"\"\"\n",
    "        Calculates gradient of the given model in learning space\n",
    "        \"\"\"\n",
    "        m = X.shape[1]\n",
    "        dw = np.dot(X,(H-Y).T)/m\n",
    "        db = np.sum(H-Y)/m\n",
    "        grads = {\"dw\": dw,\n",
    "                 \"db\": db}\n",
    "        return grads\n",
    " \n",
    "    def gradient_position(self, w, b, X, Y):\n",
    "        \"\"\"\n",
    "        It just gets calls various functions to get status of learning model\n",
    "        Arguments:\n",
    "         w -- weights, a numpy array of size (no. of features, 1)\n",
    "         b -- bias, a scalar\n",
    "         X -- data of size (no. of features, number of examples)\n",
    "         Y -- true \"label\" vector (containing 0 or 1 ) of size (1, number of examples)\n",
    "        \"\"\"\n",
    "  \n",
    "        m = X.shape[1]\n",
    "        H = self.hypothesis(w,X,b)         # compute activation\n",
    "        cost = self.cost(H,Y,m)               # compute cost\n",
    "        grads = self.cal_gradient(w, H, X, Y) # compute gradient\n",
    "        \n",
    "        return grads, cost\n",
    "    \n",
    "    def gradient_descent(self, w, b, X, Y, print_cost = False):\n",
    "        \"\"\"\n",
    "        This function optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "        Arguments:\n",
    "        w — weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "        b — bias, a scalar\n",
    "        X -- data of size (no. of features, number of examples)\n",
    "        Y -- true \"label\" vector (containing 0 or 1 ) of size (1, number of examples)\n",
    "        print_cost — True to print the loss every 100 steps\n",
    "\n",
    "        Returns:\n",
    "        params — dictionary containing the weights w and bias b\n",
    "        grads — dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "        costs — list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "        \"\"\"\n",
    " \n",
    "        costs = []\n",
    " \n",
    "        for i in range(self.num_iterations):\n",
    "        # Cost and gradient calculation \n",
    "            grads, cost = self.gradient_position(w,b,X,Y)\n",
    " \n",
    " \n",
    "            # Retrieve derivatives from grads\n",
    "            dw = grads['dw']\n",
    "            db = grads['db']\n",
    " \n",
    "            \n",
    "            # update rule \n",
    "            w = w - (self.learning_rate * dw) \n",
    "            b = b - (self.learning_rate * db)\n",
    " \n",
    "            # Record the costs\n",
    "            if i % 100 == 0:\n",
    "                costs.append(cost)\n",
    " \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost and i % 100 == 0:\n",
    "                 print ('Cost after iteration %i: %f' %(i, cost))\n",
    " \n",
    " \n",
    "        params = {'w': w,\n",
    "                  'b': b}\n",
    " \n",
    "        grads = {'dw': dw,\n",
    "                 'db': db}\n",
    " \n",
    "        return params, grads, costs\n",
    "\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "        Arguments:\n",
    "        w -- weights, a numpy array of size (n, 1)\n",
    "        b -- bias, a scalar\n",
    "        X -- data of size (num_px * num_px * 3, number of examples)\n",
    "\n",
    "        Returns:\n",
    "        Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "        '''\n",
    "        \n",
    "        X = np.array(X)\n",
    "        m = X.shape[1]\n",
    "  \n",
    "        Y_prediction = np.zeros((1,m))\n",
    "  \n",
    "        w = self.w.reshape(X.shape[0], 1)\n",
    "        b = self.b\n",
    "        # Compute vector \"H\" \n",
    "        H = self.hypothesis(w, X, b)\n",
    " \n",
    "        for i in range(H.shape[1]):\n",
    "        # Convert probabilities H[0,i] to actual predictions p[0,i]\n",
    "            if H[0,i] >= 0.5:\n",
    "                Y_prediction[0,i] = 1\n",
    "            else: \n",
    "                Y_prediction[0,i] = 0\n",
    "   \n",
    "        return Y_prediction\n",
    "\n",
    "    def train_model(self, X_train, Y_train, X_test, Y_test, print_cost = False):\n",
    "        \"\"\"\n",
    "        Builds the logistic regression model by calling the function you’ve implemented previously\n",
    "\n",
    "        Arguments:\n",
    "        X_train — training set represented by a numpy array of shape (features, m_train)\n",
    "        Y_train — training labels represented by a numpy array (vector) of shape (1, m_train)\n",
    "        X_test — test set represented by a numpy array of shape (features, m_test)\n",
    "        Y_test — test labels represented by a numpy array (vector) of shape (1, m_test)\n",
    "        print_cost — Set to true to print the cost every 100 iterations\n",
    "\n",
    "        Returns:\n",
    "        d — dictionary containing information about the model.\n",
    "        \"\"\"\n",
    "        # initialize parameters with zeros \n",
    "        dim = np.shape(X_train)[0]\n",
    "        w, b = self.initialize_weight(dim)\n",
    "        # Gradient descent \n",
    "        parameters, grads, costs = self.gradient_descent(w, b, X_train, Y_train, print_cost = False)\n",
    " \n",
    "        # Retrieve parameters w and b from dictionary “parameters”\n",
    "        self.w = parameters['w']\n",
    "        self.b = parameters['b']\n",
    " \n",
    "        # Predict test/train set examples \n",
    "        Y_prediction_test = self.predict(X_test)\n",
    "        Y_prediction_train = self.predict(X_train)\n",
    "        # Print train/test Errors\n",
    "        train_score = 100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100\n",
    "        test_score = 100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100\n",
    "        print('train accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print('test accuracy: {} %'.format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "        d = {'costs': costs,\n",
    "             'Y_prediction_test': Y_prediction_test, \n",
    "             'Y_prediction_train' : Y_prediction_train, \n",
    "             'w' : self.w, \n",
    "             'b' : self.b,\n",
    "             'learning_rate': self.learning_rate,\n",
    "             'num_iterations': self.num_iterations,\n",
    "             'train accuracy': train_score,\n",
    "             'test accuracy': test_score}\n",
    " \n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing on a small dataset\n",
    "#Dataset\n",
    "X_train = np.array([[5,6,1,3,7,4,10,1,2,0,5,3,1,4],[1,2,0,2,3,3,9,4,4,3,6,5,3,7]])\n",
    "Y_train = np.array([[0,0,0,0,0,0,0,1,1,1,1,1,1,1]])\n",
    "X_test  = np.array([[2,3,3,3,2,4],[1,1,0,7,6,5]])\n",
    "Y_test  = np.array([[0,0,0,1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 100.0 %\n",
      "test accuracy: 100.0 %\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "clf = MyLogisticRegression()\n",
    "d = clf.train_model(X_train, Y_train, X_test, Y_test)\n",
    "print (d[\"train accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
